{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Sequencing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGgrcL8sgtmCuGnzqyP5o6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdicherlaVenkataSai/NLP-Zero-to-Hero/blob/master/2.%20Sequencing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3ApbDH4GHaU",
        "colab_type": "text"
      },
      "source": [
        "# Sequencing - Turning sentences into data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw9d93ZgFxtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lCQKFr-M_kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "             'i love my dog',\n",
        "             'i love my cat ',\n",
        "             'you love my dog',\n",
        "             'do you think my dog is amazing?'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECuVbO_8NSQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "1e3b325a-7b4e-4fe4-f803-f2c5fa2ccadd"
      },
      "source": [
        "#creating an instance of it\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "#tokenizer gets the word index from the sentences and create it as sequence\n",
        "word_index = tokenizer.word_index\n",
        "word_index"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'amazing': 10,\n",
              " 'cat': 6,\n",
              " 'do': 7,\n",
              " 'dog': 3,\n",
              " 'i': 4,\n",
              " 'is': 9,\n",
              " 'love': 2,\n",
              " 'my': 1,\n",
              " 'think': 8,\n",
              " 'you': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVVJsLErNlDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "08c1f8eb-41c0-4c04-b40d-e84c41010bef"
      },
      "source": [
        "#sentences are converted into sequence of numbers\n",
        "# tokemizer has a method texts_to_sequence \n",
        "# it creates sequences of tokens representing each sentence\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "sequences\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv7ryVumVjIo",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfgeIpLoOf_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = [\n",
        "        'i really love my dog',\n",
        "        'my dog loves my manatee'\n",
        "]\n",
        "#these test data contains words which doesnt have tokens\n",
        "#and aren't present in word index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8kM7XXiQe3K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "daa720c4-cc2e-4a01-e976-d8a997ffa37d"
      },
      "source": [
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "test_seq"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 2, 1, 3], [1, 3, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umRtLsKlQvA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the sequences are of diff length when compared it with the sentence\n",
        "#that becauses words like (really, loves, manatee) is not avail in word_index\n",
        "# we really need big word_index to handle the word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CwUtTdSVmb0",
        "colab_type": "text"
      },
      "source": [
        "#OOV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU_20XWRRYxk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "85a47db2-61eb-402d-9778-5d3e04a2aa00"
      },
      "source": [
        "#in order to not to lose the length of sequence,\n",
        "#using OOV token property and setting it as something\n",
        "#that you would not expect to see in the corpus like <OOV>\n",
        "#the tokenizer will create a token for that and\n",
        "#replace work which is doesnt recoginze with the Out Of Vocabulary token instead\n",
        "\n",
        "\n",
        "tokenizer1 = Tokenizer(num_words=100, oov_token='<OOV>')\n",
        "tokenizer1.fit_on_texts(sentences)\n",
        "word_index = tokenizer1.word_index\n",
        "#print(word_index)\n",
        "sequences1 = tokenizer1.texts_to_sequences(sentences)\n",
        "\n",
        "test_data = [\n",
        "        'i really love my dog',\n",
        "        'my dog loves my manatee'\n",
        "]\n",
        "test_seq = tokenizer1.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBWWNlw7StAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we have achieved the length , but still lost some meaning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NxkDfG0ToWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# in neural networks to handle the sizes of different lengths\n",
        "# RaggedTensor\n",
        "#simpler sol is padding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plz-CGu4Va0c",
        "colab_type": "text"
      },
      "source": [
        "#Padding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_NqToPbVcNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ZocmtUV1Yl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "3754da35-abf0-48c9-a845-44dc40f489f7"
      },
      "source": [
        "# sentences\n",
        "# tokenizer 1 instance\n",
        "# test_data\n",
        "# test_seq\n",
        "\n",
        "padded = pad_sequences(sequences)\n",
        "print(word_index, '\\n')\n",
        "print(sequences, '\\n')\n",
        "print(padded)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11} \n",
            "\n",
            "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]] \n",
            "\n",
            "[[ 0  0  0  4  2  1  3]\n",
            " [ 0  0  0  4  2  1  6]\n",
            " [ 0  0  0  5  2  1  3]\n",
            " [ 7  5  8  1  3  9 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTlLWyxkWK4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "841ca2e1-30b1-4c99-8e71-5e2def4beb07"
      },
      "source": [
        "# the overall length will be equal to longest sentence available  here =7\n",
        "#  note: <OOV> : 1 not 0, only padded bits are 0\n",
        "# if we want to pad at the end\n",
        "padded = pad_sequences(sequences, padding = 'post')\n",
        "print(padded)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 4  2  1  3  0  0  0]\n",
            " [ 4  2  1  6  0  0  0]\n",
            " [ 5  2  1  3  0  0  0]\n",
            " [ 7  5  8  1  3  9 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1kZcDADXa-6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "3189a385-66b4-41de-d91a-3132e4de6496"
      },
      "source": [
        "# if we dont want the padded sequneces with equal length to longest sentence, ex as len = 5\n",
        "# truncating = pre/ post it coulld either cut off from begining or end\n",
        "padded = pad_sequences(sequences, padding = 'post', maxlen = 5, truncating = 'post')\n",
        "print(padded)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4 2 1 3 0]\n",
            " [4 2 1 6 0]\n",
            " [5 2 1 3 0]\n",
            " [7 5 8 1 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}