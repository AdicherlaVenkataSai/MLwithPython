{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5. Generalization notes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNUO9umUC/W/L81p6WrEuY1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdicherlaVenkataSai/GooglecrashML/blob/master/5.%20Generalization_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCa2s3_olry",
        "colab_type": "text"
      },
      "source": [
        "#Generalization \n",
        "\n",
        "it refers to your model's ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMJHK-UqolzL",
        "colab_type": "text"
      },
      "source": [
        "Three basic assumptions in all of the above:\n",
        "\n",
        "We draw examples independently and identically (i.i.d.) at random from the distribution\n",
        "The distribution is stationary: It doesn't change over time\n",
        "We always pull from the same distribution: Including training, validation, and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfqSBcCLprPp",
        "colab_type": "text"
      },
      "source": [
        "An overfit model gets a low loss during training but does a poor job predicting new data. If a model fits the current sample well, how can we trust that it will make good predictions on new data?\n",
        "\n",
        "overfitting is caused by making a model more complex than necessary. The fundamental tension of machine learning is between fitting our data well, but also fitting the data as simply as possible.\n",
        "\n",
        "Machine learning's goal is to predict well on new data drawn from a (hidden) true probability distribution. Unfortunately, the model can't see the whole truth; the model can only sample from a training data set. If a model fits the current examples well, how can you trust the model will also make good predictions on never-before-seen examples?\n",
        "\n",
        "William of Ockham, a 14th century friar and philosopher, loved simplicity. He believed that scientists should prefer simpler formulas or theories over more complex ones. To put Ockham's razor in machine learning terms:\n",
        "The less complex an ML model, the more likely that a good empirical result is not just due to the peculiarities of the sample.\n",
        "\n",
        "In modern times, we've formalized Ockham's razor into the fields of statistical learning theory and computational learning theory. These fields have developed generalization bounds--a statistical description of a model's ability to generalize to new data based on factors such as:\n",
        "\n",
        "the complexity of the model\n",
        "the model's performance on training data\n",
        "While the theoretical analysis provides formal guarantees under idealized assumptions, they can be difficult to apply in practice. Machine Learning Crash Course focuses instead on empirical evaluation to judge a model's ability to generalize to new data.\n",
        "\n",
        "A machine learning model aims to make good predictions on new, previously unseen data. But if you are building a model from your data set, how would you get the previously unseen data? Well, one way is to divide your data set into two subsets:\n",
        "\n",
        "training set—a subset to train a model.\n",
        "test set—a subset to test the model.\n",
        "Good performance on the test set is a useful indicator of good performance on the new data in general, assuming that:\n",
        "\n",
        "The test set is large enough.\n",
        "You don't cheat by using the same test set over and over.\n",
        "The ML fine print\n",
        "The following three basic assumptions guide generalization:\n",
        "\n",
        "We draw examples independently and identically (i.i.d) at random from the distribution. In other words, examples don't influence each other. (An alternate explanation: i.i.d. is a way of referring to the randomness of variables.)\n",
        "The distribution is stationary; that is the distribution doesn't change within the data set.\n",
        "We draw examples from partitions from the same distribution.\n",
        "In practice, we sometimes violate these assumptions. For example:\n",
        "\n",
        "Consider a model that chooses ads to display. The i.i.d. assumption would be violated if the model bases its choice of ads, in part, on what ads the user has previously seen.\n",
        "Consider a data set that contains retail sales information for a year. User's purchases change seasonally, which would violate stationarity.\n",
        "When we know that any of the preceding three basic assumptions are violated, we must pay careful attention to metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMDe5jQPpwYr",
        "colab_type": "text"
      },
      "source": [
        "###summary\n",
        "Overfitting occurs when a model tries to fit the training data so closely that it does not generalize well to new data.\n",
        "If the key assumptions of supervised ML are not met, then we lose important theoretical guarantees on our ability to predict on new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPQR_FPwpvw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}